
\section*{Introduction}

Seasonal influenza virus infects 5--15\% of the global population every year causing an estimated 250,000 to 500,000 deaths annually \cite{flufactsheet}.
Vaccination remains the most effective public health response available.
However, frequent viral mutation results in viruses that escape previously acquired human immunity.
The World Health Organization (WHO) selects vaccine viruses to match circulating viruses, but because the process of vaccine development and distribution requires several months to complete, accurate vaccine strain selection requires a prediction of which viruses will predominate approximately one year after vaccine viruses are selected.
Current vaccine predictions focus on the hemagglutinin (HA) protein, which acts as the primary target of human immunity.
The hemagglutination inhibition (HI) assay \cite{hirst1943studies} is used to measure the degree of cross-reactivity between pairs of circulating viruses.
HI assays are fundamental for vaccine strain selection, but they are laborious and low-throughput compared to genome sequencing \cite{Wood:2012ii}.
As a result, researchers have developed computational methods to predict influenza fitness from sequence data alone \cite{Luksza:2014hj,Steinbruck:2014kq,Neher:2014eu}.

Despite the promise of these sequence-only models, they explicitly omit experimental measurements of antigenic or functional phenotypes.
Recent developments in computational methods and influenza virology have made it feasible to integrate these important metrics of influenza fitness into a single predictive model.
For example, phenotypic measurements of antigenic drift are now accessible through phylogenetic models \cite{Neher:2016hy} and functional phenotypes for HA are available from deep mutational scanning experiments \cite{Lee2018}.
We describe an approach to integrate previously disparate sequence-only models of influenza evolution with high-quality experimental measurements of antigenic drift and functional constraint.

The influenza community has long recognized the importance of incorporating HI phenotypes and other experimental measurements of viral phenotypes with existing forecasting methods into an extensible, open source framework that can be used by professional virologists in their vaccine design process \cite{Gandon:2016gz,Morris:2017ea,Lassig:2017hr}.
Although several distinct efforts have made progress in using HI phenotypes to evaluate the evolution of seasonal influenza \cite{Steinbruck:2014kq,Neher:2016hy}, these methods stop short of developing a complete forecasting framework wherein the evolutionary contribution of HI phenotypes can be compared and contrasted with new and existing fitness metrics.
Here, we provide the first such open source long-term forecasting framework for seasonal influenza.
With this framework, we show that HI phenotypes enable more accurate long-term forecasts of A/H3N2 populations compared to previous metrics based on epitope mutations alone.
However, we also find that phylogenetic fitness metrics based on recent growth of circulating clades consistently outperform any combination of genotypic or phenotypic metrics, suggesting that existing mechanistic models of seasonal influenza evolution are missing critical components.

\section*{Results}

\subsection*{A distance-based model of seasonal influenza evolution}

\begin{figure*}[ht]
  \begin{center}
  \includegraphics[width=\columnwidth]{figures/distance-based-fitness-model.png}
  \caption{
    Schematic representation of the fitness model for simulated A/H3N2-like populations wherein the fitness of samples at timepoint $t$ determines the estimated frequency of samples with similar sequences one year in the future at timepoint $u$.
    Samples are colored by their amino acid sequence composition such that genetically similar samples have similar colors.
    A) Samples at timepoint $t$ are shown in their phylogenetic context and sized by their frequency at that timepoint.
    The estimated future population at timepoint $u$ is projected to the right with samples scaled in size by their projected frequency based on the true fitness model.
    B) The frequency trajectories of samples at timepoint $t$ to $u$ represent the predicted the growth of the orange samples to the detriment of the purple samples.
    C) Samples at timepoint $u$ are shown in the corresponding phylogeny for that timepoint and scaled by their frequency at that time.
    D) The observed frequency trajectories of samples at timepoint $u$ broadly recapitulate the model's forecasts while also revealing increased diversity of sequences at the future timepoint that the model could not anticipate, e.g. the emergence of the yellow cluster from within the successful orange cluster.
  }
  \label{fig:model}
  \end{center}
\end{figure*}

Here, we present a model of seasonal influenza evolution inspired by the Malthusian growth fitness model of {\L}uksza and L\"assig \cite{Luksza:2014hj}.
As with this original model, we seek to forecast the frequencies of viral populations one year in advance by applying to each virus sample an exponential growth factor scaled by an estimate of the sample's fitness (Fig.~\ref{fig:model} and Eq.~\ref{equation_exponential_growth_model}).
We estimate the frequency of virus samples every six months using a smoothed KDE kernel to represent the frequency of each sample.

We estimate viral fitness with biologically-informed metrics including those originally defined by \cite{Luksza:2014hj} of epitope cross-immunity and mutational load (non-epitope mutations) as well as four more recent metrics including hemagglutination inhibition (HI) cross-immunity \cite{Neher:2016hy}, deep mutational scanning (DMS) mutational effects \cite{Lee2018}, local branching index (LBI) \cite{Neher:2014eu}, and change in clade frequency over time (delta frequency).
All of these metrics except for HI cross-immunity and DMS mutational effects rely only on HA sequences.
The cross-immunity metrics estimate how antigenically distinct each sample at time $t$ is from previously circulating samples based on either genetic distance at epitope sites or $\log_{2}$ titer distance from HI measurements.
Increased antigenic drift relative to previously circulating samples is expected to correspond to increased viral fitness.
Mutational load is the simplest of the functional constraint metrics, measuring the number of putatively deleterious mutations that have accumulated in each sample since their ancestor in the previous season.
DMS mutational effects provide a more comprehensive biophysical model of functional constraint by measuring the beneficial or deleterious effect of each possible single amino acid mutation in HA from the background of a previous vaccine strain, A/Perth/16/2009.
The growth metrics estimate how successful populations of samples have been in the last six months based on either rapid branching in the phylogeny (LBI) or the change in clade frequencies over time (delta frequency).

We fit models by learning coefficients for each fitness metric either individually or in linear combinations from training data and select the best of these models using time-series cross-validation.
After selecting optimal models from training and validation, we evaluated the true out-of-sample errors of these models on additional data that were held out from the initial model fitting and tuning.
Importantly, our models find fitness coefficients that minimize the normalized average Hamming distance between the observed population one year in the future and the estimated population produced by the fitness model (Fig.~\ref{fig:model}).
With this approach, we avoid the intrinsic instability of clade definitions due to variability in phylogenetic reconstruction from year to year.
However, we retain the benefits of fitting models to highly similar samples found within clades and enable future forecasting efforts for pathogens whose sequences are not amenable to standard phylogenetic inference.

\subsection*{Models accurately forecast evolution of A/H3N2-like viruses}

The long-term evolution of influenza A/H3N2 hemagglutinin has been previously described as a balance between positive selection for substitutions that enable escape from adaptive immunity by modifying existing epitopes and purifying selection on domains that are required to maintain the protein's primary functions of binding and membrane fusion \cite{Bush:1999vj,Neher2013,Luksza:2014hj,Koelle:2015dh}.
To test the ability of our models to accurately detect these evolutionary patterns under controlled conditions, we simulated the long-term evolution of A/H3N2-like viruses under positive and purifying selection for 40 years (Methods, Supplemental Fig.~\ref{sup_fig:cross_validation_for_simulated_populations}).
These selective constraints produced phylogenetic structures and accumulation of epitope and non-epitope mutations that were consistent with phylogenies of natural A/H3N2 HA (Supplemental Fig. \ref{sup_fig:simulated_h3n2_ha_phylogeny}, Supplemental Tables~\ref{sup_table:mutations_by_trunk_status_for_simulated_populations} and \ref{sup_table:mutations_by_trunk_status}).

We fit models to these simulated populations using all sequence-only fitness metrics.
As a positive control for our model framework, we also fit a model based on the true fitness of each sample as measured by the simulator.
We evaluated the performance of each model by the absolute distance between projected and observed future populations and how often it outperformed a naive model that estimated no change in populations between years.
As each model only projects existing sequences forward one year without trying to predict which mutations occur in the future, the model's absolute distance to the future can never be zero.
For this reason, we additionally measured how well each model approximated the diversity of the future population.
We measured this approximation at each timepoint as the average Hamming distance between projected and observed populations minus the average Hamming distance of the observed population to itself.
\jhc{Insert summary statistics of future diversity without describing average Hamming distance and remove approximation of future diversity text.}

\begin{figure*}[ht!]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/unadjusted-model-accuracy-and-coefficients-for-simulated-populations-controls.png}
  \caption{
    Model a) coefficients and b) distances between projected and observed future populations as measured in amino acids (AAs) for simulated populations.
    Coefficients are shown per validation timepoint (solid circles, N=33) with the mean $\pm$ standard deviation in the top-left corner.
    For model testing, coefficients were fixed to their mean values from training/validation (open circles, N=18) and applied to out-of-sample test data.
    Distances between projected and observed populations are shown per validation timepoint (solid black circles) or test timepoint (open black circles).
    The mean $\pm$ standard deviation of distances per validation timepoint are shown in the top-left of each panel.
    Corresponding values per test timepoint are in the top-right.
    For reference, the naive model's distance to the future is also shown per validation and test timepoint in grey.
    The naive model's distance to the future was 8.97 $\pm$ 1.35 AAs for validation timepoints and 9.07 $\pm$ 1.71 AAs for test timepoints.
  }
  \label{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations_controls}
  \end{center}
\end{figure*}

We hypothesized that fitness metrics associated with viral success such as epitope cross-immunity, LBI, and delta frequency would be assigned positive coefficients, while metrics associated with fitness penalties, like mutational load, would receive negative coefficients.
We reasoned that both LBI and delta frequency would individually outperform the mechanistic metrics as both of these growth metrics estimate recent clade success regardless of the mechanistic basis for that success.
Correspondingly, we expected that a composite model of epitope cross-immunity and mutational load would perform as well as or better than the growth metrics, as this model would include both primary fitness constraints acting on our simulated populations.

\begin{table*}[ht]
  \begin{center}
    \scalebox{0.9}{
      \input{tables/simulated_model_selection.tex}
    }
    \caption{
      Model coefficients and performance on validation and test data for simulated populations ordered from best to worst by distance to the future in the validation analysis.
      Coefficients are the mean $\pm$ standard deviation for each metric in a given model across 33 training windows.
      Distance to the future (mean $\pm$ standard deviation) measures the distance in amino acids between estimated and observed future populations.
      The number of times (and percentage of total times) each model outperformed the naive model measures the benefit of each model over a model than estimates no change between current and future populations.
      Test results are based on 18 timepoints not observed during model training and validation.
    }
    \label{table_simulated_model_selection}
  \end{center}
\end{table*}

As expected, the true fitness model outperformed all other models, estimating a future population within 6.82 $\pm$ 1.52 amino acids (AAs) of the observed future and surpassing the naive model in 32 of 33 (97\%) timepoints (Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations_controls}, Table~\ref{table_simulated_model_selection}).
Importantly, the true fitness model accurately approximated the diversity of future populations coming within -0.29 $\pm$ 2.01 AAs of observed future diversity.
In contrast to the true fitness model, the naive model was an average of 8.97 $\pm$ 1.35 AAs from the future and an average of 2.26 $\pm$ 1.75 AAs from the future's observed diversity.
With the exception of epitope cross-immunity, all biologically-informed models consistently outperformed the naive model (Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations}, Table~\ref{table_simulated_model_selection}).
LBI was the best of these models, with a distance to the future of 7.57 $\pm$ 1.85 AAs.
Indeed, both growth-based models received positive coefficients and outperformed the mechanistic models.
The mutational load metric received a consistently negative coefficient with an average distance of 8.27 $\pm$ 1.35 AAs.

\begin{figure*}[ht!]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/unadjusted-model-accuracy-and-coefficients-for-simulated-populations.png}
  \caption{
    Model a) coefficients and b) distances to the future for individual biologically-informed fitness metrics and the best composite model fit to simulated populations.
    Coefficients and distances are shown per validation and test timepoint as in Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations_controls}.
    The naive model's distance to the future was 8.97 $\pm$ 1.35 AAs for validation timepoints and 8.90 $\pm$ 1.69 AAs for test timepoints.
  }
  \label{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations}
  \end{center}
\end{figure*}

Surprisingly, the composite model of epitope cross-immunity and mutational load did not perform better than the individual mutational load model (Supplemental Fig.~\ref{sup_fig:unadjusted_composite_model_accuracy_and_coefficients_for_simulated_populations}).
While epitope cross-immunity was not predictive on its own, the consistently positive coefficient it received in the composite model suggested that it was an informative metric when effects of purifying selection were also represented in the model.
The cross-immunity fitness metric assumes that antigenic drift is driven by nonlinear effects of previous host exposure \cite{Luksza:2014hj} that are not explicitly present in our simulations.
To understand whether positive selection at epitope sites might be better represented by a linear model, we fit an additional model based on an ``epitope ancestor'' metric that counted the number of epitope mutations since each sample's ancestor in the previous season.
This linear fitness metric did not outperform the cross-immunity metric (Table~\ref{table_simulated_model_selection}).
However, when we fit a composite model of the epitope ancestor and mutational load metrics, this model outperformed both the individual mutational load model and the composite cross-immunity and mutational load model (Supplemental Fig.~\ref{sup_fig:unadjusted_composite_model_accuracy_and_coefficients_for_simulated_populations}).
From these results, we concluded that our method can accurately estimate the evolution of simulated populations, but that the fitness of simulated samples was dominated by purifying selection and only weakly affected by a linear effect of positive selection at epitope sites.

We hypothesized that a composite model of mutually beneficial metrics could better approximate the true fitness of simulated viruses than models based on individual metrics.
To this end, we fit an additional model including the best metrics from the mechanistic and clade growth categories: mutational load and LBI.
This composite model outperformed all individual metrics with an average distance to the future of 7.24 $\pm$ 1.66 AAs and outperforming the naive model as often as the true fitness metric (Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations}).
The coefficients for mutational load and LBI remained relatively consistent across all validation timepoints, indicating that these fitness metrics were stable approximations of the simulator's underlying evolutionary processes.
These results support our hypothesis that multiple complementary metrics can produce more accurate models.

\begin{figure*}[p]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/test-of-best-model-for-simulated-populations.png}
  \caption{
  Test of best model for simulated populations (true fitness) using 9 years previously unobserved test data and fixed model coefficients.
  A) The correlation of estimated and observed clade growth rates shows the model's ability to capture clade-level dynamics without explicitly optimizing for clade frequency targets.
  B) The rank of the estimated best strain based on its distance to the future in the best model for 18 timepoints shows how often the model makes a good choice when forced to select a single representative strain for the future population.
  The estimated best strain was in the top 20th percentile of observed closest strains for 89\% of timepoints, confirming that the model makes a good choice when forced to select a single representative strain for the future population.
  C) Absolute forecast error for clades shown in A by their initial frequency with a mean LOESS fit (solid black line) and 95\% confidence intervals (grey shading) based on 100 bootstraps.
  D) The correlation of all strains at all timepoints by the percentile rank of their observed and estimated distances to the future.
  }
  \label{fig:test_of_best_model_for_simulated_populations}
  \end{center}
\end{figure*}

We validated the best performing model (true fitness) using two metrics that are relevant for practical influenza forecasting and vaccine design efforts.
First, we measured the ability of the true fitness model to accurately estimate dynamics of large clades (initial frequency $>15\%$) by correlating estimated and observed clade growth rates.
Our model recapitulated clade dynamics with a Pearson's correlation of $R^2 = 0.52$~$(p < 0.001)$ between observed and estimated growth rates and overall accuracies for clade growth and decline predictions of 87\% and 58\%, respectively (Supplemental Fig.~\ref{sup_fig:validation_of_best_model_for_simulated_populations}A).
As expected, our model produced more accurate forecasts for clades with higher initial frequencies (Supplemental Fig.~\ref{sup_fig:validation_of_best_model_for_simulated_populations}C).
Next, we counted how often the estimated closest strain to the future population at any given timepoint ranked among the observed top closest strains to the future.
The estimated best strain was in the top 1st percentile of observed closest strains for half of the validation timepoints and in the top 20th percentile for 100\% of timepoints (Supplemental Fig.~\ref{sup_fig:validation_of_best_model_for_simulated_populations}B).
Percentile ranks per strain based on their observed and estimated distances to the future correlated strongly across all strains and timepoints (Spearman's $\rho^2 = 0.87$, $p < 0.001$, Supplemental Fig.~\ref{sup_fig:validation_of_best_model_for_simulated_populations}D).

Finally, we tested all of our models on out-of-sample data.
Specifically, we fixed the coefficients of each model to the average values across the training/validation period and applied the resulting models to the next 9 years of previously unobserved simulated data.
A standard expectation from machine learning is that models will perform worse on test data than on validation data.
Instead, we found that nearly all models except for the true fitness and delta frequency performed as well or better with out-of-sample data (Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations_controls}, Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations}, Supplemental Fig.~\ref{sup_fig:unadjusted_composite_model_accuracy_and_coefficients_for_simulated_populations}).
The composite model of mutational load and LBI outperformed the true fitness metric with average distance to the future of 7.10 $\pm$ 1.19 compared to 7.38 $\pm$ 1.89, respectively (Supplemental Table~\ref{table_simulated_model_selection}).
Both the composite and true fitness models outperformed the naive model at 100\% and 89\% of test timepoints, respectively.

As with our validation dataset, we tested the true fitness model's ability to recapitulate clade dynamics and select optimal individual strains from the test data.
We observed a growth rate correlation of $R^2 = 0.14$~$(p < 0.001)$ and clade growth and decline prediction accuracies of 82\% and 53\%, respectively (Supplemental Fig.~\ref{fig:test_of_best_model_for_simulated_populations}A).
Correspondingly, we observed higher absolute forecast errors in the test data with higher errors for clades between 40\% and 60\% initial frequencies (Supplemental Fig.~\ref{fig:test_of_best_model_for_simulated_populations}C).
The estimated best strain was higher than the top first percentile of observed closest strains for half of the test timepoints and in the top 20th percentile for 16 of 18 (89\%) of timepoints (Supplemental Fig.~\ref{fig:test_of_best_model_for_simulated_populations}B).
Observed and estimated strain ranks remained strongly correlated (Spearman's $\rho^2 = 0.80$, $p < 0.001$, Supplemental Fig.~\ref{fig:test_of_best_model_for_simulated_populations}D).
These results confirm that our approach of minimizing the distance between yearly populations can simulataneously capture clade-level dynamics of these populations and identify optimal individual strains as vaccine candidates.
These results also highlight a well-established yet critical point that true forecasts on out-of-sample data are less accurate than those based on training and validation data.

\subsection*{Models reflect historical patterns of A/H3N2 evolution}

\begin{table*}[ht]
  \begin{center}
    \scalebox{0.9}{
      \input{tables/natural_model_selection.tex}
    }
    \caption{
      Model coefficients and performance on validation and test data for natural populations ordered from best to worst by distance to the future, as in Table~\ref{table_simulated_model_selection}.
      Validation results are based on 23 timepoints.
      Test results are based on 8 timepoints not observed during model training and validation.
    }
    \label{table_natural_model_selection}
  \end{center}
\end{table*}

Next, we trained and validated models for individual fitness predictors using 25 years of natural A/H3N2 populations spanning from October 1, 1990 to October 1, 2015.
We held out samples collected between October 1, 2015 and October 1, 2019 for model testing (Supplemental Fig.~\ref{sup_fig:cross_validation_for_natural_populations}).
In addition to the sequence-only models we tested on simulated populations, we also fit models for our new fitness metrics based on experimental phenotypes including HI cross-immunity and DMS mutational effects.
We hypothesized that both HI and DMS metrics would be assigned positive coefficients, as they estimate increased antigenic drift and beneficial mutations, respectively.
As antigenic drift is generally considered to be the primary evolutionary pressure on natural A/H3N2 populations \cite{Smith:2004jc,Bedford:2014bf,Luksza:2014hj}, we expected that epitope and HI cross-immunity would be individually more predictive than mutational load or DMS mutational effects.
Previous research \cite{Neher:2016hy} and our simulation results also led us to expect that LBI and delta frequency would outperform other individual mechanistic metrics.

\begin{figure*}[ht]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/unadjusted-model-accuracy-and-coefficients-for-natural-populations.png}
  \caption{
    Model a) coefficients and b) distances to the future for individual biologically-informed fitness metrics fit to natural populations.
    Coefficients and distances are shown per validation timepoint (N=23) and test timepoint (N=8) as in Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations_controls}.
    The naive model's distance to the future was 6.40 $\pm$ 1.36 AAs for validation timepoints and 6.82 $\pm$ 1.74 AAs for test timepoints.
  }
  \label{fig:unadjusted_model_accuracy_and_coefficients_for_natural_populations}
  \end{center}
\end{figure*}

The naive model measured a distance of 6.40 $\pm$ 1.36 AAs between natural populations or 71\% of the distance between yearly simulated populations (Table~\ref{table_natural_model_selection}).
Biologically-informed metrics generally performed better than the naive model for natural populations with the exceptions of the epitope cross-immunity and DMS mutational effects (Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_natural_populations}).
The average improvement of the sequence-only models over the naive model was consistently lower than the same models in simulated populations.
This reduced performance may have been caused by both the relatively reduced diversity between years in natural populations and the increased complexity of the fitness constraints on these real populations.

Of the two metrics for antigenic drift, HI cross-immunity consistently outperformed epitope cross-immunity (Table~\ref{table_natural_model_selection}).
HI cross-immunity estimated an average distance to the future of 6.04 $\pm$ 1.57 AAs and outperformed the naive model at 17 of 23 timepoints (74\%).
After the first three training windows where HI measurements were relatively sparse, the HI cross-immunity coefficient was stable across all remaining timepoints (Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_natural_populations}).
In contrast, epitope cross-immunity estimated a distance of 7.06 $\pm$ 1.37 AAs and only outperformed the naive model at six timepoints (26\%).
Epitope cross-immunity was also the only metric whose coefficient started at a positive value (1.45 $\pm$ 0.10 on average prior to October 2009) and transitioned to a negative value through the validation period (-0.13 $\pm$ 0.32 on average for October 2009 and after).
This strong coefficient for the first half of training windows indicated that, unlike the results for simulated populations, the nonlinear cross-immunity metric was historically an effective measure of antigenic drift.
The historical importance of the epitope sites used for this metric was further supported by the relative enrichment of mutations at these sites for the most successful ``trunk'' lineages of natural populations compared to side branch lineages (Supplemental Table~\ref{sup_table:mutations_by_trunk_status}).

These results led us to hypothesize that the contribution of these specific epitope sites to antigenic drift has weakened over time.
Importantly, these 49 epitope sites were originally selected by {\L}uksza and L\"assig \cite{Luksza:2014hj} from a previous historical survey of sites with beneficial mutations between 1968--2005 \cite{Shih:2007bd}.
If the beneficial effects of mutations at these sites were due to historical contingency rather than a constant contribution to antigenic drift, we would expect models based on these sites to perform well until 2005 and then overfit relative to future data.
Indeed, the epitope cross-immunity model is strongly predictive for the first two validation timepoints until it has to predict to October 2005.
To further test our hypothesis, we identified a new set of beneficial sites across our entire validation period of October 1990 through October 2015.
Inspired by the original approach of Shih et al. \cite{Shih:2007bd}, we identified 25 sites in HA1 where mutations rapidly swept through the global population, including 12 that were also present in the original set of 49 sites.
We fit a cross-immunity model to these 25 sites across the complete validation period and dubbed this the ``oracle cross-immunity'' model, as it benefited from knowledge of the future in its forecasts.
In contrast to the epitope cross-immunity model, the oracle cross-immunity model produced a strong positive coefficient across all training windows (1.06 $\pm$ 0.23) and consistently outperformed the original model with an average distance to the future of 5.59 $\pm$ 1.30 AAs (Supplemental Fig.~\ref{sup_fig:unadjusted_composite_model_accuracy_and_coefficients_for_natural_populations_epitope_vs_oracle}).
These results support our hypothesis that the success of previous epitope models based on a fixed set of 49 sites is due to historical contingency rather than a constant contribution of these sites to antigenic drift.
We suspect that our HI cross-immunity model benefits from its ability to constantly update its antigenic model at each timepoint with recent experimental phenotypes, while the epitope cross-immunity metric is forced to give a constant weight to the same 49 sites throughout time.

Of the two metrics for functional constraint, mutational load outperformed DMS mutational effects, with an average distance to the future of 6.14 $\pm$ 1.37 AAs compared to 6.75 $\pm$ 1.95 AAs, respectively.
In contrast to the original {\L}uksza and L\"assig \cite{Luksza:2014hj} model, where the coefficient of the mutational load metric was fixed at -0.5, our model learned a consistently stronger coefficient of -0.99 $\pm$ 0.30.
Notably, the best performance of the DMS mutational effects model was forecasting from April 2007 to April 2008 when the major clade containing A/Perth/16/2009 was first emerging.
This result is consistent with the DMS model overfitting to the evolutionary history of the background strain used to perform the DMS experiments.
Alternate implementations of less background-dependent DMS metrics never performed better than the mutational load metric (Supplemental Table~\ref{sup_table:complete_natural_model_selection}).
Thus, we find that a simple model where any mutation at non-epitope sites is deleterious is more predictive of global viral success than a more comprehensive biophysical model based on measured mutational effects of a single strain.

LBI was the best individual fitness metric by average distance to the future (Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_natural_populations}) and tied HI cross-immunity and mutational load by outperforming the naive model at 17 (74\%) timepoints (Table~\ref{table_natural_model_selection}).
Delta frequency performed worse than LBI and HI cross-immunity and only performed slightly better than mutational load.
While delta frequency should, in principle, measure the same aspect of viral fitness as LBI, these results clearly show that the current implementations of these metrics represent qualitatively different fitness components.

\begin{figure*}[ht]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/best-composite-unadjusted-model-accuracy-and-coefficients-for-natural-populations.png}
  \caption{
    Model a) coefficients and b) distances to the future for composite fitness metrics fit to natural populations.
    Coefficients and distances are shown per validation timepoint (N=23) and test timepoint (N=8) as in Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_simulated_populations_controls}.
  }
  \label{fig:unadjusted_composite_model_accuracy_and_coefficients_for_natural_populations}
  \end{center}
\end{figure*}

To test whether composite models could outperform individual fitness metrics for natural populations, we fit models based on combinations of best individual metrics representing antigenic drift, functional constraint, and clade growth.
Specifically, we fit models based on HI cross-immunity and mutational load, mutational load and LBI, and all three of these metrics together.
We anticipated that if these metrics all represented distinct, mutually beneficial components of viral fitness, these composite models should perform better than individual models with consistent coefficients for each metric.

Both two-metric composite models performed better than their corresponding individual models (Table~\ref{table_natural_model_selection} and Fig.~\ref{fig:unadjusted_composite_model_accuracy_and_coefficients_for_natural_populations}).
Although the three-metric composite model performed slightly worse than the individual LBI model based on average distance to the future, this composite model outperformed the naive model the most of any model with 19 (83\%) times out of 23.
The composite of mutational load and LBI performed the best overall with an average distance to the future of 5.44 $\pm$ 1.80 AAs.

The stability of the coefficients for the metrics in the two-metric models suggested that these metrics represented complementary components of viral fitness.
In contrast, the three-metric model strongly preferred the HI cross-immunity and non-epitope metrics over LBI for the majority of the validation period, producing an average LBI coefficient of 0.05 $\pm$ 0.50.
In the last two validation timepoints, this composite model converged to a nearly LBI-only model with the coefficients for the other two metrics converging to near-zero values.
These last two timepoints span the period when an HA1:160T substitution swept through the global population.
This substitution introduced a new glycosylation motif and dramatically reduced the efficiency of HI assays \cite{Zost2017}.
These results reinforce the historical importance of HI assays in measuring viral fitness, while also indicating that these assays may no longer be as predictive as sequence-only metrics.

As with the simulated populations, we validated the performance of the best model for natural populations using estimated and observed clade growth rates and the ranking of estimated best strains compared to the observed closest strains to future populations.
The composite model of mutational load and LBI effectively captured clade dynamics with a growth correlation of $R^2 = 0.35$~$(p < 0.001)$ and growth and decline accuracies of 87\% and 89\%, respectively (Supplemental Fig.~\ref{sup_fig:validation_of_best_model_for_natural_populations}A).
Absolute forecasting error declined noticeably for clades with initial frequencies above 60\%, but generally this error remained below 20\% on average (Supplemental Fig.~\ref{sup_fig:validation_of_best_model_for_natural_populations}C).
The estimated best strain from this model was in the top first percentile of observed closest strains for half of the validation timepoints and in the top 20th percentile for 20 (87\%) of 23 timepoints (Supplemental Fig.~\ref{sup_fig:validation_of_best_model_for_natural_populations}B).
This pattern held across all strains and timepoints with a strong correlation between observed and estimated strain ranks (Spearman's $\rho^2 = 0.66$, $p < 0.001$, Supplemental Fig.~\ref{sup_fig:validation_of_best_model_for_natural_populations}D).

\begin{figure*}[p]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/test-of-best-model-for-natural-populations.png}
  \caption{
  Test of best model for natural populations of A/H3N2 viruses, the composite model of HI cross-immunity, mutational load, and LBI.
  A) The correlation of estimated and observed clade growth rates shows the model's ability to capture clade-level dynamics without explicitly optimizing for clade frequency targets.
  B) The rank of the estimated best strain based on its distance to the future for 8 timepoints.
  The estimated best strain was in the top 20th percentile of observed closest strains for 88\% of timepoints.
  C) Absolute forecast error for clades shown in A by their initial frequency with a mean LOESS fit (solid black line) and 95\% confidence intervals (grey shading) based on 100 bootstraps.
  D) The correlation of all strains at all timepoints by the percentile rank of their observed and estimated distances to the future.
  }
  \label{fig:test_of_best_model_for_natural_populations}
  \end{center}
\end{figure*}

Finally, we tested the performance of all models on out-of-sample data collected from October 1, 2015 through October 1, 2019.
We anticipated that most models would perform worse on truly out-of-sample data than on validation data.
As expected, all models estimated higher average distances to the future across these eight test timepoints (Table~\ref{table_natural_model_selection}).
However, these increases were mostly consistent with an increased average distance between years of 6.82 $\pm$ 1.74 AAs as measured by the naive model.
The composite of HI cross-immunity, mutational load, and LBI outperformed all other models on the test data with an average distance of 6.12 $\pm$ 1.42 AAs.
Surprisingly, the best model for the validation data -- mutational load and LBI -- was one of the worst models for the test data with an average distance to the future of 7.70 $\pm$ 3.53 AAs.
The individual LBI model was the worst model, while mutational load continued to perform well with test data.
LBI performed especially poorly in the last two test timepoints of April and October 2018 (Fig.~\ref{fig:unadjusted_model_accuracy_and_coefficients_for_natural_populations}).
These timepoints correspond to the dominance and sudden decline of a reassortant clade named A2/re \cite{Potter2019}.
By April 2018, the A2/re clade had risen to a global frequency over 50\% from less than 15\% the previous year, despite an absence of antigenic drift.
However, by October 2018, this clade had declined in frequency to approximately 30\% and, by October 2019, it had gone extinct.
These results highlight a major limitation of growth-based fitness metrics like LBI and a corresponding benefit of more mechanistic metrics that explicitly measure antigenic drift and functional constraint.

After identifying the composite HI cross-immunity, mutational load, and LBI model as the best model on out-of-sample data, we tested this model's ability to detect clade dynamics and select individual best strains for vaccine composition.
The composite model partially captured clade dynamics with a Pearson's correlation of $R^2 = 0.50$~$(p < 0.001)$ between observed and estimated growth rates and growth and decline accuracies of 57\% and 58\%, respectively (Fig.~\ref{fig:test_of_best_model_for_natural_populations}A).
The mean absolute forecasting error with this model was consistently less than 20\%, regardless of the initial clade frequency (Fig.~\ref{fig:test_of_best_model_for_natural_populations}C).
The estimated best strain from this model was in the top second percentile of observed closest strains for half of the validation timepoints and in the top 20th percentile for seven (88\%) of eight timepoints (Fig.~\ref{fig:test_of_best_model_for_natural_populations}B).
Similarly, the observed and estimated strain ranks strongly correlated (Spearman's $\rho^2 = 0.69$, $p < 0.001$) across all strains and test timepoints (Fig.~\ref{fig:test_of_best_model_for_natural_populations}D).

\begin{figure*}[ht]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/vaccine-comparison.png}
  \caption{
    Observed distance to natural A/H3N2 populations one year into the future for each vaccine strain (green) and the observed (blue) and estimated (orange) closest strains to the future at the corresponding timepoints.
    Vaccine strains were assigned to their closest timepoint in the validation or test time periods and their distance to the future was calculated from their amino acid sequences and the frequencies and sequences of the corresponding population one year in the future.
    The estimated closest strain to the future was identified by either the best model in the validation period (mutational load and LBI) or the best model in the test period (HI cross-immunity, mutational load, and LBI).
  }
  \label{fig:vaccine_comparison}
  \end{center}
\end{figure*}

We further evaluated our best model's apparent success at estimating the closest strain to the next season's A/H3N2 population by comparing our model's selection to the WHO's vaccine strain selection.
For each season when the WHO selected a new vaccine strain and one year of future data existed in our validation or test periods, we measured the observed distance of that strain's sequence to the future and the corresponding distances to the future for the observed and estimated closest strains.
Our models selected a strain that was closer to the future than the selected vaccine strain for 10 (83\%) of the 12 seasons with vaccine updates (Fig.~\ref{fig:vaccine_comparison}).
In the two seasons where the vaccine strain was closer to the future than the model's selection, the difference between the two strains' distances to the future was less than one amino acid.

\subsection*{Forecasts predict the rise of A1b/131K and A1b/135K sub-clades in September 2020}

To enable real-time forecasts, we integrated our forecasting framework into our existing open source pathogen surveillance application, Nextstrain \cite{Hadfield2018}, and used our best model to forecast A/H3N2 populations in September 2020.
Our combined HI cross-immunity, mutational load, and LBI model favored the growth of the currently circulating clade A1b/197R, the maintenance of A1b/137F, and the decline of 3c3.A (Fig.~\ref{fig:nextstrain_forecasts}).
To aid with identification of potential vaccine candidates, we annotated samples in the phylogeny by their estimated distance to the future based on our best model (Fig.~\ref{fig:nextstrain_distance_to_future}).

\begin{figure*}[ht]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/nextstrain-forecasts-for-september-2020.png}
  \caption{
    Snapshot of live forecasts on nextstrain.org from our best model for September 2020.
    The observed frequency trajectories for currently circulating clades are shown up to September 2019.
    Our model favors the A1b/131K subclade, A1b/197R, to grow over the next year and estimates that the A1b/135K subclade, A1b/137F, will remain stable.
  }
  \label{fig:nextstrain_forecasts}
  \end{center}
\end{figure*}

\begin{figure*}[ht]
  \begin{center}
  \includegraphics[width=\textwidth]{figures/nextstrain-weighted-distance-to-future-per-strain.png}
  \caption{
    Snapshot of the last two years of seasonal influenza A/H3N2 evolution on nextstrain.org showing the estimated distance per sample to the future population.
    Distance to the future is calculated for each sample as the Hamming distance of HA amino acid sequences to all other circulating samples weighted by the other sample's projected frequencies under the best fitness model.
  }
  \label{fig:nextstrain_distance_to_future}
  \end{center}
\end{figure*}

\section*{Discussion}

Key findings:

\begin{itemize}
\item{Our model accurately forecasts clade frequency trajectories and identifies optimal potential vaccine strains for both simulated and natural populations by estimating the sequence composition of future populations and without relying on clade-based model targets}
\item{Experimental measurements of antigenic drift based on HI assays are more predictive of viral success than epitope mutations}
\item{Models based on a predefined set of epitope sites risk overfitting to historical data}
\item{The combination of sequence-only metrics outperforms all experimentally-informed metrics across validation data but fails to account for reassortment events in test data}
\item{Deleterious mutations contribute more to seasonal influenza evolution than has been as widely appreciated}
\item{DMS measurements appear to be too background-specific to be predictive for global viral success}
\item{Further efforts to understand the declining efficacy of HI assays and to replace these with FRA or antigenic escape assays could improve models in the future}
\item{Our model live forecasts in nextstrain.org will aid in year-round surveillance of influenza evolutionary patterns and allow us to continuously evaluate model performance relative to recent observations}
\item{Our model is the first of its kind to be released as an open source framework that can be inspected and extended by others}
\item{Immediate next steps to improve influenza models under this framework include the integration of geographic information and antigenic escape assay data}
\item{Region-aware LBI could immediate integrate geography into models to account for issues like the A2/re forecast errors}
\item{Our distance-based model targets and easy definition of fitness metrics through tidy data frames paves the way for future forecasting efforts with pathogens that cannot be analyzed by standard phylogenetic methods (e.g., highly recombinant viruses and organisms with larger genomes like bacteria and fungi)}
\end{itemize}

\section*{Methods}

\subsection*{Simulation of influenza A/H3N2-like populations}

We simulated the long-term evolution of A/H3N2-like viruses with SANTA-SIM \cite{Jariani2019} for 10,000 generations or 50 years where 200 generations was equivalent to 1 year.
We discarded the first 10 years as a burn-in period, selected the next 30 years for model fitting and validation, and held out the last 9 years as out-of-sample data for model testing.
Each simulated population was seeded with the full length HA from A/Beijing/32/1992 (NCBI accession: U26830.1) such that all simulated sequences contained signal peptide, HA1, and HA2 domains.
We defined purifying selection across all three domains, allowing the preferred amino acid at each site to change at a fixed rate over time.
We additionally defined exposure-dependent selection for 49 putative epitope sites in HA1 \cite{Luksza:2014hj} to impose an effect of cross-immunity that would allow mutations at those sites to increase viral fitness despite underlying purifying selection.
We modified the SANTA-SIM source code to enable the inclusion of true fitness values for each sample in the FASTA header of the sampled sequences from each generation.
This modified implementation is available at \url{https://github.com/huddlej/santa-sim/tree/emit-fitness}.
For our full analysis of model performance, we sampled 90 viruses per month to match the sampling density of natural populations.
For tuning of hyperparameters, we sampled 10 viruses per month to enable rapid exploration of hyperparameter space.

\subsection*{Hyperparameter tuning with simulated populations}

To avoid overfitting our models to the relatively limited data from natural populations, we used simulated A/H3N2-like populations to tune hyperparameters including the KDE bandwidth for frequency estimates and the L1 penalty for model coefficients.
We simulated populations, as described above, and fit models for each parameter value using the true fitness of samples from the simulator.

We identified the optimal KDE bandwidth for frequencies as the value that minimized the difference between the mean distances to the future from the true fitness model and the naive model.
We set the L1 lambda penalty to zero, to reduce variables in the analysis and avoid interactions between the coefficients and the KDE bandwidths.
Higher bandwidths completely wash out dynamics of populations by making all samples appear to exist for long time periods.
This flattening of frequency trajectories means that as bandwidths increase, the naive model gets more accurate and less informative.
Given this behavior, we found the bandwidth that produced the minimum difference between distances to the future for the true fitness and naive models instead of the bandwidth that produced the minimum mean model distance.
Based on this analysis, we identified an optimal bandwidth of $\frac{2}{12}$ or the equivalent of 2-months for floating point dates.
Next, we identified an L1 penalty of 0.1 for model coefficients that minimized the mean distance to the future for the true fitness model.

\subsection*{Strain selection for natural populations}

For model training and validation, we selected 13,601 HA sequences $\geq$900 nucleotides that were sampled between October 1, 1990 and October 1, 2015.
To account for known variation in sequence availability by region, we subsampled the selected sequences to a representative set of 90 viruses per month with even sampling across 10 global regions including Africa, Europe, North America, China, South Asia, Japan and Korea, Oceania, South America, Southeast Asia, and West Asia.
We excluded all egg-passaged samples and all samples with ambiguous year, month, and day annotations.
We prioritized samples with more available HI titer measurements.
For model testing, we selected an additional 20,803 HA sequences corresponding to 90 viruses per month sampled between October 1, 2015 and October 1, 2019.
This increased number of sequences corresponds with the trend of increased sample sequencing and deposition in the last decade.
We used these test sequences to evaluate the out-of-sample error of fixed model parameters learned during training and validation.

\subsection*{Phylogenetic inference}

For each timepoint in model training, validation, and testing, we selected the subsampled HA sequences with collection dates up to that timepoint.
We aligned sequences with the augur align command \cite{Hadfield2018} and MAFFT v7.407 \cite{Katoh2002}.
We inferred initial phylogenies for HA sequences at each timepoint with IQ-TREE v1.6.10 \cite{Nguyen2014}.
To reconstruct time-resolved phylogenies, we applied TreeTime v0.5.6 \cite{Sagulenko2018} with the augur refine command.

\subsection*{Frequency estimation}

To account for uncertainty in collection date and sampling error, we applied a kernel density estimation (KDE) approach to calculate global sample frequencies.
Specifically, we constructed a Gaussian kernel for each sample with the mean at the reported collection date and a variance (or KDE bandwidth) of two months.
The bandwidth was identified by cross-validation, as described above.
This bandwidth also roughly corresponds to the median lag time between sample collection and submission to the GISAID database.
We estimated the frequency of each sample at each timepoint by calculating the probabilitiy density function of each KDE at that timepoint and normalizing the resulting values to sum to one.
We implemented this frequency estimation logic in the augur frequencies command.

\subsection*{Model fitting and evaluation}

\subsubsection*{Fitness model}

We assumed that the evolution seasonal influenza A/H3N2 populations can be represented by a Malthusian growth fitness model, as previously described \cite{Luksza:2014hj}.
Under this model, we estimated the future frequency, $\hat{x}_{i}(t + \Delta{t})$, of each sample $i$ from the sample's current frequency, $x_{i}(t)$, and fitness, $f_{i}(t)$, as follows where the resulting future frequencies were normalized to one by $\frac{1}{Z(t)}$.

\begin{equation}
    \hat{x}_{i}(t + \Delta{t}) = \frac{1}{Z(t)}x_{i}(t)\exp(f_{i}(t)\Delta{t})
    \label{equation_exponential_growth_model}
\end{equation}

We defined the fitness of each sample at time $t$ as the additive combination of one or more fitness metrics, $f_{i,m}$, scaled by fitness coefficients, $\beta_{m}$.
For example, Equation~\ref{equation_fitness_estimation} estimates fitness per sample by mutational load ($\mathrm{ml}$) and local branching index ($\mathrm{lbi}$).

\begin{equation}
    f_{i}(t) = \beta_{\mathrm{ne}}f_{i, \mathrm{ml}}(t) + \beta_{\mathrm{lbi}}f_{i, \mathrm{lbi}}(t)
    \label{equation_fitness_estimation}
\end{equation}

\subsubsection*{Model target}

For a model based on any given combination of fitness metrics, we found the fitness coefficients that minimized the weighted Hamming distance between amino acid sequences from the observed future population at time $u = t + \Delta{t}$ and the estimated future population created by projecting frequencies of samples at time $t$ by their estimated fitnesses.
We measured distance between populations using Earth Mover's Distance (EMD), a metric commonly applied in machine learning to compare collections of pixels or words \cite{Rubner1998,Kusner2015}.
Solving for EMD identifies the minimum about of ``earth'' that must be moved from a source population to a sink population to make those populations as similar as possible.
This solution requires both a ``ground distance'' between pairs fo samples from both populations and weights assigned to each sample that determine how much that sample contributes to the overall distance.

For each timepoint $t$ and corresponding timepoint $u = t + 1$, we defined the ground distance as the Hamming distance between HA amino acid sequences for all pairs of samples between timepoints.
For samples with less than full length nucleotide sequences, we inferred missing nucleotides through TreeTime's ancestral sequence reconstruction analysis.
We defined weights for samples at timepoint $t$ based on their projected future frequencies.
We defined weights for samples at timepoint $u$ based on their observed frequencies.
We then identified the fitness coefficients that provided projected future frequencies that minimized the EMD between the estimated and observed future populations.
With this metric, an optimal estimate of the future would produce a distance of zero.
However, the inevitable accumulation of substitutions between the two populations prevents this outcome.
We calculated EMD with the Python bindings for the OpenCV 3.4.1 implementation \cite{opencv_library}.
We applied the Nelder-Mead minimization algorithm as implemented in SciPy \cite{SciPy} to learn fitness coefficients that minimize the average of this distance metric over all timepoints in a given training window.

\subsubsection*{Time-series cross-validation}

To obtain unbiased estimates for the out-of-sample errors of our models, we adopted the standard cross-validation strategy of training, validation, and testing.
We divided our available data into an initial training and validation set spanning October 1990 to October 2015 and an additional testing set spanning October 2015 to October 2019.
We partitioned our training and validation data into six month seasons corresponding to winter in the Northern Hemisphere (October--April) and the Southern Hemisphere (April--October) and trained models to estimate frequencies of populations one year into the future from each season in six-year sliding windows.
To calculate validation error for each training window, we applied the resulting model coefficients to estimate the future frequencies for the year after the last timepoint in the training window.
These validation errors informed our tuning of hyperparameters including a L1 regularization of the fitness coefficients, the LBI neighborhood parameter $\tau$, and the length of the training window itself.
Finally, we fixed the coefficients for each model at the mean values across all training windows and applied these fixed models to the test data to estimate the true forecasting accuracy of each model on previously unobserved data.

\subsection*{Fitness metrics}

We defined the following fitness metrics per strain and timepoint.

\subsubsection*{Antigenic drift}

We estimated antigenic drift for each strain using either genetic or HI data.
To estimate antigenic drift with genetic data, we implemented the cross-immunity metric originally defined by {\L}uksza and L\"assig \cite{Luksza:2014hj}.
Briefly, for each pair of strains in adjacent seasons, we counted the number of amino acid differences between the strains' HA sequences at 49 epitope sites.
The one-based coordinates of these sites relative to the start of the HA1 segment were 50, 53, 54, 121, 122, 124, 126, 131, 133, 135, 137, 142, 143, 144, 145, 146, 155, 156, 157, 158, 159, 160, 163, 164, 172, 173, 174, 186, 188, 189, 190, 192, 193, 196, 197, 201, 207, 213, 217, 226, 227, 242, 244, 248, 275, 276, 278, 299, and 307.
We limited pairwise comparisons to all strains sampled within the last five years from each timepoint.
For each individual strain $i$ at each timepoint $t$, we estimated that strain's ability to escape cross-immunity by summing the exponentially-scaled epitope distances between previously circulating strains and the given strain as follows.

\begin{equation}
    f_{i,\mathrm{ep}}(t) = \sum_{j: t_{j} < t_{i}}{x_{j}\exp{(-D_{\mathrm{ep}}(a_{i}, a_{j}) / D_{0})}}
    \label{equation_epitope_cross_immunity}
\end{equation}

To test the historical contingency of the epitope sites defined above, we additionally identified a new set of sites with beneficial mutations across the training/validation period of October 1990 through October 2015.
Following the general approach of Shih et al. \cite{Shih:2007bd}, we manually identified 25 sites in HA1 where mutations rapidly swept through the global population.
We required mutations to emerge from below 5\% global frequency and reach $>$90\% frequency.
Although we did not require sweeps to complete within a fixed amount of time, we observed that they required no longer than 1-3 years to complete.
To minimize false positives, we eliminated any sites where one or more mutations rose above 20\% frequency and subsequently died out.
If two or more sites had redundant sweep dynamics (mutations emerging and fixing at the same times), we retained the site with the most mutational sweeps.
Based on this requirements, we defined our final collection of ``oracle'' sites in HA1 coordinates as 3, 45, 48, 50, 75, 140, 145, 156, 158, 159, 173, 186, 189, 193, 198, 202, 212, 222, 223, 225, 226, 227, 278, 311, and 312.

To estimate antigenic drift with HI data, we first applied the titer tree model to the phylogeny at a given timepoint and the corresponding HI data for its strains, as previously described by Neher et al. 2016 \cite{Neher:2016hy}.
This method effectively estimates the antigenic drift per branch in units of $log_{2}$ titer change.
Next, we calculated the pairwise antigenic distance between strains as the sum of antigenic drift weights per branch on the phylogenetic path between each pair of strains.
Finally, we calculated each strain's ability to escape cross-immunity using Equation~\ref{equation_epitope_cross_immunity} with the pairwise distances between epitope sequences replaced with pairwise antigenic distance from HI data.
As with the original epitope cross-immunity described above, this HI cross-immunity metric produces higher values for strains that are more antigenically distinct from previously circulating strains.

\subsubsection*{Functional constraint}

We estimated functional constraint for each strain using either genetic or deep mutational scanning (DMS) data.
To estimate functional constraint with genetic data, we implemented the non-epitope mutation metric originally defined by {\L}uksza and L\"assig \cite{Luksza:2014hj}.
This metric counts the number of amino acid differences at 517 non-epitope sites in HA sequences between each strain $i$ at timepoint $t$ and that strain's most recent inferred ancestral sequence in the previous season ($t - 1$).

We estimated functional constraint using mutational preferences from DMS data as previously defined \cite{Lee2018}.
Briefly, mutational effects were defined as the log ratio of DMS preferences, $\pi$, at site $r$ for the derived amino acid, $a_{i}$, and the ancestral amino acid, $a_{j}$.
As with the non-epitope mutation metric above, we considered only substitutions in HA between each strain $i$ and that strain's most recent inferred ancestral sequence in the previous season.
We calculated the total effect of these substitutions as the sum of the mutational preferences for each substitution, as in Equation~\ref{equation_mutational_preference}.

\begin{equation}
    f_{i,\mathrm{DMS}}(t) = \sum_{r \in r,a_{i} != r,a_{j}}\log_{2}\frac{\pi_{r,a_{i}}}{\pi_{r,a_{j}}}
    \label{equation_mutational_preference}
\end{equation}

\subsubsection*{Clade growth}

We estimated clade growth for each strain using local branching index (LBI) and the change in frequency over time (delta frequency).
To calculate LBI for each strain at each timepoint, we applied the LBI heuristic algorithm as originally described \cite{Neher:2014eu} to the phylogenetic tree constructed at each timepoint.
We set the neighborhood parameter, $\tau$, to 0.3 and only considered viruses sampled in the last 6 months of each phylogeny as contributing to recent clade growth.

We estimated the change in frequency over time by calculating clade frequencies under a Brownian motion diffusion process as previously described \cite{Lee2018}.
These frequency calculations allowed us to assign a partial clade frequency to each strain within nested clades.
We calculated the delta frequency as the change in frequency for each strain between the most recent timepoint in a given phylogeny and six months prior to that timepoint divided by 0.5 years.
